<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- iOS Safari -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <!-- Chrome, Firefox OS and Opera Status Bar Color -->
  <meta name="theme-color" content="#FFFFFF">
  <meta property="og:title" content="Latent Dirichlet Allocation">
  
  <meta property="og:type" content="blog">
  <title>Latent Dirichlet Allocation</title>
  <!-- Favicon -->
  
  <link rel="shortcut icon" href="ğŸ¤–">
  
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
  <link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
  <link rel="stylesheet" type="text/css" href="css/notablog.css">
  <link rel="stylesheet" type="text/css" href="css/theme.css">
  <style>
    :root {
      font-size: 18px;
    }

    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
    <a href="index.html">
      <div class="Navbar__Btn"><span>ğŸ¤–</span> <span>Home</span></div>
    </a>
    
    
    <span class="Navbar__Delim">&centerdot;</span>
    <a href="portfolio.html">
      <div class="Navbar__Btn"><span>ğŸ’¼</span> <span>Portfolio</span></div>
    </a>
    
    
    
    <span class="Navbar__Delim">&centerdot;</span>
    <a href="blog.html">
      <div class="Navbar__Btn"><span>âœï¸</span> <span>Blog</span></div>
    </a>
    
    
    
    <span class="Navbar__Delim">&centerdot;</span>
    <a href="about.html">
      <div class="Navbar__Btn"><span>ğŸ¤“</span> <span>About</span></div>
    </a>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </nav>
  <header class="Header">
      
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
    <div class="Header__Icon"><span>ğŸ“</span></div>
    
    <h1 class="Header__Title">Latent Dirichlet Allocation</h1>
        
  </header>
      <article id="https://www.notion.so/602d80764c6d4902910b648f714cc1de" class="PageRoot"><div id="https://www.notion.so/390112e4c83741e5b8aac62673924705" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Author: Ali Arsalan Yaqoob</span></span></p></div><div id="https://www.notion.so/c43ee21b8b984d3280cc47d4c743a259" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/efad40f6b061493c84dd0f9306ea26a7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">For this post, I wanted to write about my favorite topic modeling algorithm called the Latent Dirichlet Allocation (LDA). Before we jump into LDA, I want to go over what topic modeling is. </span></span></p></div><h1 id="https://www.notion.so/444b97d5311b4d5194c128959dbed50a" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/444b97d5311b4d5194c128959dbed50a"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Topic Modeling</span></span></h1><div id="https://www.notion.so/b655a22f503f4226a41244de8782bd7c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Topic modeling is an unsupervised machine learning technique which is used for extracting hidden semantic relationships within large corpora of documents. It is unsupervised because there are no labels provided to create these models. A &quot;topic&quot; consists of a group of words that frequently occur together in documents. Each document consists of a distribution of topics (more on this later).</span></span></p></div><div id="https://www.notion.so/e834b812f0f14fd5b28ffec17ee16289" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/9677bcb6f577437e8abf3b338d26d643" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This is a powerful technique as it can allow us to discover and search unlabeled text data (basically,  everything on the internet), cluster similar documents together and also understand thematic structure and underlying abstract topics that don&#x27;t really have a word for them but can be understood what they mean when they occur together. </span></span></p></div><h2 id="https://www.notion.so/21369f87ba0c4282a7bcaa524578f939" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/21369f87ba0c4282a7bcaa524578f939"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Amazon Reviews (Example)</span></span></h2><div id="https://www.notion.so/b7a1180ba7d24e8eb2cca6727b2f0bd1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Topic modeling works by grouping similar word patterns to identify topics within a document. An example of topic models being used is on customer reviews on Amazon. </span></span></p></div><div id="https://www.notion.so/1387eb2427bb41548b72dfa43e1a3bae" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/589da99d340542bda219be8cc80a7447" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fabc3f783-25ef-4530-8e8d-537d7845e4ae%2FUntitled.png?width=672&amp;table=block&amp;id=589da99d-3405-42bd-a219-be8cc80a7447"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fabc3f783-25ef-4530-8e8d-537d7845e4ae%2FUntitled.png?width=672&amp;table=block&amp;id=589da99d-3405-42bd-a219-be8cc80a7447" style="width:672px"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Screenshot of customer review section of the book - Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems (Must read!)</span></span></figcaption></figure></div><div id="https://www.notion.so/6c3b9dd05509484bb4a2d1c2804f9fb4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Recurring word patterns or phrases are used to identify the topics that were predominantly mentioned in the reviews and these allow the reviewer to filter through the set of reviews by selecting a phrase that sounds closer to what they would be looking for. </span></span></p></div><h1 id="https://www.notion.so/d9d254132d6f43ad929e4eb009218cf8" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/d9d254132d6f43ad929e4eb009218cf8"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">LDA </span></span></h1><h2 id="https://www.notion.so/6fe858787c744ff387dcdbcfff6a0072" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/6fe858787c744ff387dcdbcfff6a0072"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Overview</span></span></h2><div id="https://www.notion.so/4c5b41bf19e34352837249350bf70276" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">LDA is one of the most popular models in topic modeling. It stands for Latent Dirichlet Allocation. Say, for example, you have a collection of news articles and you want to sort these news articles by what is the topics that exists within these news articles. Let&#x27;s say that the news articles could have three different topics - Science, Politics or Sports. LDA organizes the documents in a geometric topic space (in this case, a triangle) in a manner that they are closer to their assigned topic in that space. Visually, this would look like a triangle with each edge being one topic. </span></span></p></div><div id="https://www.notion.so/c026eefdd6104ba9874185f2d82e4bc9" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe633d4a7-8702-4043-b765-bec8038d6d4e%2FUntitled.png?width=528&amp;table=block&amp;id=c026eefd-d610-4ba9-8741-85f2d82e4bc9"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fe633d4a7-8702-4043-b765-bec8038d6d4e%2FUntitled.png?width=528&amp;table=block&amp;id=c026eefd-d610-4ba9-8741-85f2d82e4bc9" style="width:528px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/f91fd4f6cfde43fe9d312b6c2c00904c" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">In the above example, we can see that the documents that predominantly talk about politics are closer to the &quot;Politics&quot; edge of the triangle. Notice also the document that talks about both science and politics, exists in the space between those two edges. If there was an article that had all the three topics, it would be in the middle of the triangle. </span></span></p></div><h3 id="https://www.notion.so/6eaa7d72fda043018e5047de10722a08" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/6eaa7d72fda043018e5047de10722a08"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Generative Vs Discriminative Models</span></span></h3><div id="https://www.notion.so/36f5a4b7537c406b8847810bc9af0ea0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">LDA is a generative statistical model. To understand that statement we must understand what it means to be a generative statistical model in contrast to a discriminative model. Informally, generative models can generate new data instances while discriminative models discriminate between different kinds of data instances. A generative model could generate photos of people that look like real people, while discriminative models could tell the difference between two people. A more statistically formal definition would define them as follows. Given a set of data instances X and a set of labels Y:</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/4113e166e37349ddb82505db6acaf48d" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Generative models capture the joint probability of P( X, Y ) or just P( X ) if there are no labels. </span></span></li><li id="https://www.notion.so/7a0e018281504bb1b66dc3b938227a2c" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Discriminative models capture the conditional probability of P( Y | X )</span></span></li></ul><div id="https://www.notion.so/4de724ea915544a7addde59114ecd205" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Generative models include the distribution of data itself and tells you how likely a given sample is. A discriminative model ignores the question of whether a given instance is likely, and just tells you how likely a specific label is to apply to the instance. </span></span></p></div><div id="https://www.notion.so/49a3defe3f7942749a3c79f27b9f4436" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Keeping this in mind, LDA assumes that a given document can be recreated given the distribution of topics in that document and the distribution of words in the topics in that document. It assumes each document can be recreated in the following fashion. </span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/f61bff330e0e49e6845eb963c89422d8" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Decide on the number of words N a document will have. </span></span></li><li id="https://www.notion.so/232116dab9a5440dbbb194bb38f6d918" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">Choose a distribution of topics for the document given a fixed set of K topics. For example, given that you can only have two topics: Science and Politics. You might have a news article talking about how the some political organization was funding some scientific research. You may say that this article is 70% &quot;Politics&quot; and 30% &quot;Science&quot;. </span></span></li><li id="https://www.notion.so/4fa4b8b5557b4c2ca00ecd188bec3c0f" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">You then generate every word in the document by:</span></span><ul class="BulletedListWrapper"><li id="https://www.notion.so/368dc9cdcdcd4230beff6ff3f9e99dd3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">First picking a topic given the specific probabilities associated with them. Taking the previous example, you may pick &quot;Politics&quot; with 70% probability and &quot;Science&quot; with 10% probability. </span></span></li><li id="https://www.notion.so/72f6c78f378a4bceb9defd64b059c0e8" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Using the topic to generate the word. For example, if we choose the &quot;Science&quot; topic, we might possibly select &quot;biological&quot; with  30% probability and &quot;soccer&quot; with 10% probability. </span></span></li></ul><div id="https://www.notion.so/3d176929b0f74ef4bab7e8f37305b7b8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></li></ol><div id="https://www.notion.so/7a61e75eb6e743c8ab0000208c7f2348" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Therefore, LDA essentially looks at the generated documents and then goes back to find the best distribution of topics to create a document that is closest to the original document. </span></span></p></div><h3 id="https://www.notion.so/5887543485644f9d8da4a2bc9a1d69d3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/5887543485644f9d8da4a2bc9a1d69d3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">The In and Outs</span></span></h3><div id="https://www.notion.so/19bd305b528c442a9053213901f17cd4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/71f6cd32f06342528e0d6b0c0383668d" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F656011b4-6b6c-43eb-a43d-9b5a749fca5e%2FUntitled.png?width=768&amp;table=block&amp;id=71f6cd32-f063-4252-8e0d-6b0c0383668d"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F656011b4-6b6c-43eb-a43d-9b5a749fca5e%2FUntitled.png?width=768&amp;table=block&amp;id=71f6cd32-f063-4252-8e0d-6b0c0383668d" style="width:768px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/cd47fbb754344689abc89f908904c0f9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The input into this algorithm is going to be a collection of text documents. The output is going to be three things:</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/d0e723699d5e43a487063a53d0455237" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Cluster of words where each cluster is one topic. It is important to note that one word can belong to several clusters. </span></span></li><li id="https://www.notion.so/b74f14fcba8e4751a5cf532641d84cb9" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">One topic has words and a distribution of the frequency of those words (For example, the topic of politics might include higher frequency of words such a president or democrat).</span></span></li><li id="https://www.notion.so/bae1fd7f29a24c18b859cb029e843a42" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Every document that is given to the algorithm has a distribution of topics associated with it.</span></span></li></ol><h3 id="https://www.notion.so/ff5674575a6646818f72cba2c799ddcb" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/ff5674575a6646818f72cba2c799ddcb"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Plate Notation</span></span></h3><div id="https://www.notion.so/2fec4415833f420f844877175b0cd726" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F62886f95-f4f9-4c75-bdb4-2f40e13eba68%2FUntitled.png?width=494&amp;table=block&amp;id=2fec4415-833f-420f-8448-77175b0cd726"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F62886f95-f4f9-4c75-bdb4-2f40e13eba68%2FUntitled.png?width=494&amp;table=block&amp;id=2fec4415-833f-420f-8448-77175b0cd726" style="width:494px"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Plate Notation for LDA</span></span></figcaption></figure></div><p id="https://www.notion.so/400da60bb50b4a7f85322a5d5dd012b1" class="Equation" data-latex="
M - \text{number of documents}
\\
N - \text{number of words per document }\\
\alpha - \text{parameter of the per document topic distribution (Dirchlet prior) (between 0 and non-infinity)}
\\
\beta - \text{parameter of the per  topic word distribution (Dirchlet prior)(between 0 and non-infinity)}\\
\theta_{i} - \text{topic distribution of the document } i\\
\varphi_{k} - \text{word distribution of topic } k\\
z_{ij} - \text{topic for the jth word in document i (multinomial)}\\
\omega_{ij} - \text{specific word (multinomial)}
"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>âˆ’</mo><mtext>numberÂ ofÂ documents</mtext><mspace linebreak="newline"></mspace><mi>N</mi><mo>âˆ’</mo><mtext>numberÂ ofÂ wordsÂ perÂ documentÂ </mtext><mspace linebreak="newline"></mspace><mi>Î±</mi><mo>âˆ’</mo><mtext>parameterÂ ofÂ theÂ perÂ documentÂ topicÂ distributionÂ (DirchletÂ prior)Â (betweenÂ 0Â andÂ non-infinity)</mtext><mspace linebreak="newline"></mspace><mi>Î²</mi><mo>âˆ’</mo><mtext>parameterÂ ofÂ theÂ perÂ topicÂ wordÂ distributionÂ (DirchletÂ prior)(betweenÂ 0Â andÂ non-infinity)</mtext><mspace linebreak="newline"></mspace><msub><mi>Î¸</mi><mi>i</mi></msub><mo>âˆ’</mo><mtext>topicÂ distributionÂ ofÂ theÂ documentÂ </mtext><mi>i</mi><mspace linebreak="newline"></mspace><msub><mi>Ï†</mi><mi>k</mi></msub><mo>âˆ’</mo><mtext>wordÂ distributionÂ ofÂ topicÂ </mtext><mi>k</mi><mspace linebreak="newline"></mspace><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><mtext>topicÂ forÂ theÂ jthÂ wordÂ inÂ documentÂ iÂ (multinomial)</mtext><mspace linebreak="newline"></mspace><msub><mi>Ï‰</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><mtext>specificÂ wordÂ (multinomial)</mtext></mrow><annotation encoding="application/x-tex">
M - \text{number of documents}
\\
N - \text{number of words per document }\\
\alpha - \text{parameter of the per document topic distribution (Dirchlet prior) (between 0 and non-infinity)}
\\
\beta - \text{parameter of the per  topic word distribution (Dirchlet prior)(between 0 and non-infinity)}\\
\theta_{i} - \text{topic distribution of the document } i\\
\varphi_{k} - \text{word distribution of topic } k\\
z_{ij} - \text{topic for the jth word in document i (multinomial)}\\
\omega_{ij} - \text{specific word (multinomial)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord text"><span class="mord">numberÂ ofÂ documents</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">numberÂ ofÂ wordsÂ perÂ documentÂ </span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">Î±</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">parameterÂ ofÂ theÂ perÂ documentÂ topicÂ distributionÂ (DirchletÂ prior)Â (betweenÂ 0Â andÂ non-infinity)</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">Î²</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">parameterÂ ofÂ theÂ perÂ topicÂ wordÂ distributionÂ (DirchletÂ prior)(betweenÂ 0Â andÂ non-infinity)</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">Î¸</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">topicÂ distributionÂ ofÂ theÂ documentÂ </span></span><span class="mord mathdefault">i</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Ï†</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">wordÂ distributionÂ ofÂ topicÂ </span></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8694379999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">topicÂ forÂ theÂ jthÂ wordÂ inÂ documentÂ iÂ (multinomial)</span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8694379999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">Ï‰</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">specificÂ wordÂ (multinomial)</span></span></span></span></span></span></p><div id="https://www.notion.so/75832c9616444ea6827bf65a81a56639" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Above is the plate notation for the LDA. This is used to represent probabilistic graphical models and allows us to visualize the dependencies among the many variables of the model. </span></span></p></div><div id="https://www.notion.so/82b0513ef0684a4fa70c94a484bd049d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This is a complicated illustration and if you are like me, it will not make a lot of sense to you at first.  I will try to abstract this as much as possible using the previous visualization so it is easier to grasp.</span></span></p></div><div id="https://www.notion.so/c54ace767e6c449a81685f480a2999e4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/8422ddbdf2d348c1b474c1014eaec43a" class="Image Image--Normal"><figure><a href="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc5b1f086-a9db-4a23-8998-610d2665aaf9%2FUntitled.png?width=768&amp;table=block&amp;id=8422ddbd-f2d3-48c1-b474-c1014eaec43a"><img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc5b1f086-a9db-4a23-8998-610d2665aaf9%2FUntitled.png?width=768&amp;table=block&amp;id=8422ddbd-f2d3-48c1-b474-c1014eaec43a" style="width:768px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/e74ca96322204a19b8c282e8370f913b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/f09b5cfab8234a7cbf2277084e65daf4" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">After adding the new labels, the concepts start becoming a lot clearer. However, we still have a lot of work to do to clearly understand LDA. We have M number of documents and each of these documents has N words. These are going to go through the LDA algorithm. After passing these through the algorithm, we get K number of topics. Here each topic has </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">â</span></span><span class="SemanticString"> distribution of words and each document has  </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">â</span></span><span class="SemanticString"> distribution of topics. For example, one document can have 10% of topic A, 20% of topic B and then 70% of topic C. </span></span></p></div><div id="https://www.notion.so/f6d8b2e0df56440d9d3f9c6ca984a52e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We can use the concentration parameters ( </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">â</span></span><span class="SemanticString"> ) to tune the model using our domain knowledge or understanding of topics per document and words per topic. Here, </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">â</span></span><span class="SemanticString"> signifies the degree to which the documents are close to the topics. </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Unknown">â</span></span><span class="SemanticString"> denotes the degree to which a particular topic lies in a space of words. For example, the topic &quot;politics&quot;, would be closer to the words &quot;parliament&quot; and &quot;president&quot; as compared to &quot;touchdown&quot; or &quot;black hole&quot;. Using these two parameters, we assign each word within a particular document to K topics of interest, and generate a probability that the word belongs to that particular topic. </span></span></p></div><h2 id="https://www.notion.so/21ceed31387742c2926750b2d19d9f93" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/21ceed31387742c2926750b2d19d9f93"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">LDA Algorithm</span></span></h2><div id="https://www.notion.so/ca016d254b4042a9b11a48db1b31e433" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Given a set of documents, we can use LDA to determine the number of words in each of those documents that are related to particular topics of our interest.  We do this through a series of steps (Gibbs sampling method):</span></span></p></div><ol class="NumberedListWrapper"><li id="https://www.notion.so/640c469f1315433daa166aeb3bfb6c13" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString">Go through each document. Randomly assign each word within that document to one of the K topics of interest that we have chosen.</span></span></li><li id="https://www.notion.so/60c57834c633432e8475acf32ffc1a11" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString">This initial random assignment gives us topic distributions of all the documents, and word distributions of all the topics. However, since this is random, these initial distributions are not very good. We need to improve these distributions. </span></span></li><li id="https://www.notion.so/b201222549fd4e00ac74f2c40831f394" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString">Therefore, we then go through each word j in each document d and compute two things for each topic K:</span></span><p id="https://www.notion.so/1612f02682b64d65852dd26a6b0c59dc" class="Equation" data-latex="P(\text{topic t | document d}) = \text{the proportion of words in document d that are currently assigned to topic t}"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>topicÂ tÂ |Â documentÂ d</mtext><mo stretchy="false">)</mo><mo>=</mo><mtext>theÂ proportionÂ ofÂ wordsÂ inÂ documentÂ dÂ thatÂ areÂ currentlyÂ assignedÂ toÂ topicÂ t</mtext></mrow><annotation encoding="application/x-tex">P(\text{topic t | document d}) = \text{the proportion of words in document d that are currently assigned to topic t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">topicÂ tÂ |Â documentÂ d</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">theÂ proportionÂ ofÂ wordsÂ inÂ documentÂ dÂ thatÂ areÂ currentlyÂ assignedÂ toÂ topicÂ t</span></span></span></span></span></span></p><div id="https://www.notion.so/a455e5d7b8c947d6ba8d0b9c426722f3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Based on concentration parameters alpha and beta, we calculate a conditional probability that tells us how closely related each word is to each topic of interest. For example, the word &#x27;plant&#x27; would have a greater probability of being related to the topic &#x27;biology&#x27; than the topic &#x27;politics&#x27;. </span></span></p></div><p id="https://www.notion.so/0708c289664b408db20129cbe33b1441" class="Equation" data-latex="P(\text{word w | topic t}) = \text{the proportion of asssignments to topic t over all documents that come from this word w}"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mtext>wordÂ wÂ |Â topicÂ t</mtext><mo stretchy="false">)</mo><mo>=</mo><mtext>theÂ proportionÂ ofÂ asssignmentsÂ toÂ topicÂ tÂ overÂ allÂ documentsÂ thatÂ comeÂ fromÂ thisÂ wordÂ w</mtext></mrow><annotation encoding="application/x-tex">P(\text{word w | topic t}) = \text{the proportion of asssignments to topic t over all documents that come from this word w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord text"><span class="mord">wordÂ wÂ |Â topicÂ t</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">theÂ proportionÂ ofÂ asssignmentsÂ toÂ topicÂ tÂ overÂ allÂ documentsÂ thatÂ comeÂ fromÂ thisÂ wordÂ w</span></span></span></span></span></span></p><div id="https://www.notion.so/cb8b33bb00694df9a76174d83af1744e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">For example, we go through all the documents, and based on the frequency of the word &#x27;plant&#x27; in those document, we decide that the document is assigned to the topic &#x27;biology&#x27;. </span></span></p></div><div id="https://www.notion.so/eec8c5a4c3e84bb38175ff917d43914b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></li><li id="https://www.notion.so/a45579d5ddd142fdb47d398a2f4364a0" class="NumberedList" value="4"><span class="SemanticStringArray"><span class="SemanticString">We then reassign each word j a new topic. This is called resampling. Here the topic is chosen with the probability p(topic t | document d) * p(word w | topic t). Based on our generative model, this is basically the probability that the topic t generated the word j. Therefore, in this step, we make the assumption that all our topic assignments except for the current word in question are correct, and we then update the assignment of the current word using our model of how the documents are generated.
</span></span></li><li id="https://www.notion.so/527bf05116914212bf79fe967a951ff4" class="NumberedList" value="5"><span class="SemanticStringArray"><span class="SemanticString">We iterate this previous step over a chosen number of times, to reach a steady state where our assignments are much better. Using these assignments, we estimate the topic distribution of each document and the word distribution of each topic.</span></span></li></ol><div id="https://www.notion.so/24cfa69ff7564ab28e031a84e38b5c50" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString"> 
At the final iteration pass, a final solution with coherent topics is obtained. This iterative cycling is a key feature of the LDA algorithm, whereby topic assignment checking is repeated for each word in each document, over the entire collection of documents multiple times. </span></span></p></div><div id="https://www.notion.so/8871d78e54744387b4979f4c5a617ece" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/cd526faf834c450f8bedded8c95f6799" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This is the first part in a series of blog posts that will examine Latent Dirichlet Allocation. The next article is going to examine the applications and the validation of LDA. </span></span></p></div><h3 id="https://www.notion.so/8161770d66804a50ad186c1d8d285de3" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/8161770d66804a50ad186c1d8d285de3"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">References and Further Reading</span></span></h3><ol class="NumberedListWrapper"><li id="https://www.notion.so/ff6ce96f1b7b477ebdf6ee99e24dbd9d" class="NumberedList" value="1"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="http://jmlr.org/papers/volume3/blei03a/blei03a.pdf">http://jmlr.org/papers/volume3/blei03a/blei03a.pdf</a></span></span></li><li id="https://www.notion.so/746d36c4585942ef9a093953a5a83c3c" class="NumberedList" value="2"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2">https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2</a></span></span></li><li id="https://www.notion.so/0730ce0504c2457f903c2d55b2148886" class="NumberedList" value="3"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://www.youtube.com/watch?v=T05t-SqKArY">https://www.youtube.com/watch?v=T05t-SqKArY</a></span></span></li></ol><div id="https://www.notion.so/30f2b353ae0b4309987aaf44e2f5f291" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/c9173ac7256946248c2249dbe1d9b1f9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/909998985c174b4e89659c84b5f8f676" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/251c36ac2bd24b5481c86f194b291506" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/dc6491f23464412a8abb4a82cbb52a6a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/bf5fdde1befb45f69c5b020eccbaa09e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/31daa58ec837469993f4daeb17596371" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/0a1429c98323454299ffc75deb033037" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/d24089397a7844f0a42ed0de443a9414" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/5f02d95f5e554c71b0bdc14b168bd06f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/2ae70f46d87f47d5b0bdc4655020894f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/f96397f84cd94a49a4c5bfe30b67c1f5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/ed7edb8a8d584b0c8dc88127bf964045" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/a173a62b4c994f00bec583149bfdbdeb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1c58a83736c0469b8b5d2005561c1eb7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/a276a4d15ac54df29911fc5982fde23b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/06638c36521d4d6599775cc6b6094855" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/bf3df6f7b3774763ad92e9d3798f787b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/8d582a6d75e94c1c87aeedcd518f9e47" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/e135e6af61f8417faccbdcdff0c35edf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/d7c0334a03f2405995613f0ab0e3d268" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/be93449a1fb64e5984ba889e46f4f73f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/76d634c61b294c67af65e2b093f0aacc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/ca75d6bb983c46a9aef4530c6dd46637" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/9972b06c3a0c4e028079661c0ac8f09f" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b909bc224d664a7980e84d5fcd3c9775" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/3e3dfa403e91467584facd8ba4afe027" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/a059f7b1bfe542f7a1261bfea98ca379" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/814e27b60c25476f9fa5a1b6397577e7" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b143b12a6ce5484fa0ddbb9edf117af1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/743ac0417a5945d49415e588256300b5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/b6cfe380a0fa4ad9a75c616e6ae900ee" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/53a15c0608bd4f73b49ae1c0a64303a0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/3ea39a507b474c5c8b2a7e3a12e0b19d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/43e68d206b6d4f88a1ca077400bdf903" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/3c38bae31c614ea9b7c4f8059c08db70" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/2017c7eb270a499793e2e96712e8957b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/aedf6c2233114257b8e4150ffc0fc8c3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/37fca40049174d4db7e1fb24cdc0183d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/0a21ba7222984a1d9280ea0196a16a6a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/a4a199f2512347188d85e72d38111f26" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/e7b1d3f6dc8d4edabf884b9734da8c75" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/5a602d53043b4e769c28bc68172c590e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/9449f3d0155e4ad19e747139cf6662f5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div></article>
  <footer class="Footer">
        <div>&copy; Ali A. Yaqoob 2019</div>
        <div>&centerdot;</div>
        <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
            rel="noopener noreferrer">Notablog</a>.
        </div>
    </footer>
</body>

</html>